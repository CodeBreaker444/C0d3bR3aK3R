I"Ç9<p>Proximal Policy Optimization (PPO) is a popular state-of-the-art Policy Gradient Method. It is supposed to learn relatively quickly and stable while being much simpler to tune, compared to other state-of-the-art approaches like TRPO, DDPG or A3C. This makes PPO often the first choice when it comes to solving a problem for the first time. PPO strongly builds on Trust Region Policy Optimization (TRPO).</p>

<p>It applies the key concepts of TRPO like Importance Sampling, that provides better data efficiency as well as an extended version of TRPOâ€™s KL penalty, that controls the update size in the optimization step. Moreover, PPO presents an alternative, simpler method called Clipped Surrogate Objective for controlling the optimization step size.</p>

<p>Letâ€™s solve the MountainCar Continuous problem using PPO. MountainCar
Continuous involves a car trapped in the valley of a mountain. It has to
apply throttle to accelerate against gravity and try to drive out of the
valley up steep mountain walls to reach a desired flag point on the top
of the mountain. This problem is very challenging, as the agent cannot
just apply full throttle from the base of the mountain and try to reach
the flag point, as the mountain walls are steep and gravity will not
allow the car to achieve sufficient enough momentum. The optimal
solution is for the car to initially go backward and then step on the
throttle to pick up enough momentum to overcome gravity and successfully
drive out of the valley. We will see that the RL agent actually learns
this trick.</p>

<p><strong><u>PPO Algorithm</u>:</strong></p>

<p><strong>Setting the neural network initializers</strong>:</p>

<p>setting the neural network parameters (we will use two hidden layers)
and the initializers for the weights and biases. Use the Xavier
initializer for the weights and a small positive value for the initial
values of the biases.</p>

<p><strong>Define theÂ PPO</strong>Â <strong>class</strong>:</p>

<p>TheÂ PPO()Â class is now defined. First, theÂ __init__()Â constructorÂ is
defined using the arguments passed to the class. Here,Â sessÂ is the
TensorFlowÂ session.</p>

<ul>
  <li>
    <p>S_DIMÂ andÂ A_DIMÂ are the state and action dimensions,
respectively.Â </p>
  </li>
  <li>
    <p>A_LRÂ andÂ C_LRÂ are the learning rates for the actor and the critic,
respectively.</p>
  </li>
  <li>
    <p>A_UPDATE_STEPSÂ andÂ C_UPDATE_STEPSÂ are the number of update steps
used for the actor and the critic.</p>
  </li>
  <li>
    <p>CLIP_METHODÂ stores the epsilon value.</p>
  </li>
</ul>

<p><strong>Define TensorFlow placeholders</strong>: We will next need to define the
TensorFlow placeholders:Â tfsÂ for the state,Â tfdc_rÂ for the discounted
rewards,Â tfaÂ for the actions, andÂ tfadvÂ for the advantage function.</p>

<p><strong>Define the critic</strong>: The critic neural network is defined next. We use
the state (<em>s<sub>t</sub></em>) placeholder,Â self.tfs, as input to the
neural network. Two hidden layers are used with
theÂ nhidden1Â andÂ nhidden2Â number of neurons and theÂ reluÂ activation
function (bothÂ nhidden1Â andÂ nhidden2Â were set toÂ 64Â previously). The
output layer has one neuron that will output the state value
functionÂ <em>V(s<sub>t</sub>)</em>, and so no activation function is used for
the output. We then compute the advantage function as the difference
between the discounted cumulative rewards, which is stored in
theÂ self.tfdc_rÂ placeholder and theÂ self.vÂ output that we just
computed. The critic loss is computed as an L2 norm and the critic is
trained using the Adam optimizer with the objective to minimize this L2
loss</p>

<p><strong>Call the</strong>Â <strong>_build_anet</strong>Â <strong>function</strong>: We define the actor using
aÂ _build_anet()Â function that will soon be specified. Specifically,
the policy distribution and the list of model parameters are output from
this function. We call this function once for the current policy and
again for the older policy. The mean and standard deviation can be
obtained fromÂ self.piÂ by calling theÂ mean()Â andÂ stddev()Â functions,
respectively</p>

<p><strong>Sample actions</strong>: From the policy distribution,Â self.pi, we can also
sample actions using theÂ sample()Â function that is part of TensorFlow
distributions</p>

<p><strong>Update older policy parameters</strong>: The older policy network parameters
can be updated using the new policy values simply by assigning the
values from the latter to the former, using
TensorFlowâ€™sÂ assign()Â function. Note that the new policy is optimized â€“
the older policy is simply a copy of the current policy, albeit from one
update cycle earlier</p>

<p><strong>Compute policy distribution ratio</strong>: The policy distribution ratio is
computed at theÂ self.tfaÂ action, and is stored inÂ self.ratio. Note that,
exponentially, the difference of logarithms of the distributions is the
same as the ratio of the distributions. This ratio is then clipped to
bound it betweenÂ <em>1-Îµ</em>Â andÂ <em>1+Îµ</em>, as explained earlier in the theory</p>

<p><strong>Compute losses</strong>: The total loss for the policy, as mentioned
previously, involves three losses that are combined when the policy and
value neural networks share weights. However, since we consider the
other setting mentioned in the theory earlier in this chapter, where we
have separate neural networks for the policy and the value, we will have
two losses for the policy optimization. The first is the minimum of the
product of the unclipped ratio and the advantage function and its
clipped analogueâ€”this is stored inÂ self.aloss. The second loss is the
Shannon entropy, which is the product of the policy distribution and its
logarithm, summed over, and a minus sign included. This term is scaled
with the hyper parameter,Â <em>c<sub>1</sub></em>Â = 0.01, and subtracted from
the loss. For the time being, the entropy loss term is set to zero, as
it also is in the PPO paper. We can consider including this entropy loss
later to see if it makes any difference in the learning of the policy.
We use the Adam optimizer. Note that we need to maximize the original
policy loss mentioned in the theory earlier in this chapter, but the
Adam optimizer has theÂ minimize()Â function, so we have included a minus
sign inÂ self.alossÂ (see the first line of the following code), as
maximizing a loss is the same as minimizing the negative of it</p>

<p><strong>Define the</strong>Â <strong>update</strong>Â <strong>function</strong>: TheÂ update()Â function is defined
next, which takes theÂ sÂ state, theÂ aÂ action, and theÂ rÂ reward as
arguments. It involves running a TensorFlow session on updating the old
policy network parameters by calling the
TensorFlowÂ self.update_oldpi_opÂ operation.Â Then, the advantage is
computed, which, along with the state and action, is used to update
theÂ A_UPDATE_STEPSÂ actor number of iterations. Then, the critic is
updated by theÂ C_UPDATE_STEPSÂ number of iterations by running a
TensorFlow session on the critic train operation</p>

<p><strong>Define the</strong>Â <strong>_build_anet</strong>Â <strong>function</strong>: We will next define
theÂ _build_anet()Â function that was used earlier. It will compute the
policy distribution, which is treated as a Gaussian (that is, normal).
It takes theÂ self.tfsÂ stateÂ placeholderÂ as input, has two hidden layers
with theÂ nhidden1Â andÂ nhidden2Â neurons, and uses theÂ reluÂ activation
function. This is then sent to two output layers with theÂ A_DIMÂ Â action
dimensionÂ number of outputs, with one representing the mean,Â mu, and the
other the standard deviation,Â sigma. Note that the mean of the actions
are bounded, and so theÂ tanhÂ activation function is used, including a
small clipping to avoid edge values; for sigma, theÂ softplusÂ activation
function is used, shifted byÂ 0.1Â to avoid zero sigma values. Once we
have the mean and standard deviations for the actions, TensorFlow
distributionsâ€™Â NormalÂ is used to treat the policy as a Gaussian
distribution. We can also callÂ tf.get_collection()Â to obtain the model
parameters, and theÂ NormalÂ distribution and the model parameters are
returned from the function</p>

<p><strong>Define the</strong>Â <strong>choose_action</strong>Â <strong>function</strong>: We also define
aÂ choose_action()Â function to sample from the policy to obtain actions</p>

<p><strong>Define the</strong>Â <strong>get_v</strong>Â <strong>function</strong>: Finally, we also define
aÂ get_v()Â function to return the state value by running a TensorFlow
session onÂ self.v</p>

<p><strong><u>Setting up the environment</u>:</strong></p>

<p><strong>Define function:</strong>Â We then define a function for reward shaping that
will give out some extra bonus rewards and penalties for good and bad
performance, respectively. We do this for encouraging the car to go
higher towards the side of the flag which is on the mountain top,
without which the learning will be slow</p>

<p>We next chooseÂ MountainCarContinuousÂ as the environment. The total
number of episodes we will train the agent for isÂ EP_MAX, and we set
this toÂ 1000. TheÂ GAMMAÂ discount factorÂ is set toÂ 0.9Â and the learning
rates toÂ 2e-4. We use a batch size ofÂ 32Â and performÂ 10Â update steps per
cycle. The state and action dimensions are obtained and stored
inÂ S_DIMÂ andÂ A_DIM, respectively. For the PPOÂ clipÂ parameter,Â epsilon,
we use a value ofÂ 0.1.Â train_testÂ is set toÂ 0Â for training the agent
andÂ 1Â for testing</p>

<p>We create a TensorFlow session and call itÂ sess. An instance of
theÂ PPOÂ class is created, calledÂ ppo. We also create a TensorFlow saver.
Then, if we are training from scratch, we initialize all the model
parameters by callingÂ tf.global_variables_initializer(), or, if we are
continuing the training from a saved agent or testing, then we restore
from theÂ ckpt/modelÂ path</p>

<p>The mainÂ for loopÂ over episodes is then defined. Inside it, we reset the
environment and also set buffers to empty lists. The terminal
Boolean,Â done, and the number of time steps,Â t, are also initialized</p>

<p>Inside the outer loop, we have the innerÂ whileÂ loop over time steps.
This problem involves short time steps during which the car may not
significantly move, and so we use sticky actions where actions are
sampled from the policy only once everyÂ 8Â time steps.
TheÂ choose_action()Â function in theÂ PPOÂ class will sample the actions
for a given state. A small Gaussian noise is added to the actions to
explore, and are clipped in theÂ -1.0Â toÂ 1.0Â range, as required for
theÂ MountainCarContinuousÂ environment. The action is then fed into the
environmentâ€™sÂ step()Â function, which will output the
nextÂ s_Â state,Â rÂ reward,Â and the terminalÂ doneÂ Boolean.
TheÂ reward_shaping()Â function is called to shape rewards. To track how
far the agent is pushing its limits, we also compute its maximum
position and speed inÂ max_posÂ andÂ max_speed, respectively</p>

<p>If we are in training mode, the state, action, and reward are appended
to the buffer. The new state is set to the current state and we proceed
to the next time step if the episode has not already terminated.
TheÂ ep_rÂ episode total rewards and theÂ tÂ time step count are also
updated</p>

<p>If we are in the training mode, if the number of samples is equal to a
batch, or if the episode has terminated, we will train the neural
networks. For this, the state value for the new state is first obtained
usingÂ ppo.get_v. Then, we compute the discounted rewards. The buffer
lists are also converted to NumPy arrays, and the buffer lists are reset
to empty lists. TheseÂ bs,Â ba, andÂ brÂ NumPy arraysÂ are then used to
update theÂ ppoÂ objectâ€™s actor and critic networks</p>

<p>If we are in testing mode, Python is paused briefly for better
visualization. If the episode has terminated, theÂ whileÂ loop is exited
with aÂ breakÂ statement. Then, we print the maximum position and speed
values on the screen, as well as write them, along with the episode
rewards, to a file calledÂ performance.txt. Once every 10 episodes, the
model is also saved by callingÂ saver.save</p>

<p><strong><u>Evaluating theÂ performance</u>:</strong></p>

<p><strong>Full throttle</strong></p>

<p>Note that we had to navigate backward first and then step on the
throttle in order to have sufficient momentum to escape gravity and
successfully drive out of the mountain valley. What if we had just
stepped on the throttle right from the first step â€“ would the car still
be able to escape?</p>

<p>We will now set the action toÂ 1.0, that is, full throttle: As is evident
from the video generated during the training, the car is unable to
escape the inexorable pull of gravity, and remains stuck at the base of
the mountain valley.</p>

<p><strong>Random throttle</strong></p>

<p>What if we try random throttle values? We set the valuesÂ with random
actions in theÂ -1.0Â toÂ 1.0Â range</p>

<p>Here too, the car fails to escape gravity and remains stuck at the base.
So, the RL agent is required to figure out that the optimum policy here
is to first go backward, and then step on the throttle to escape gravity
and reach the flag on the mountain top</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nearing flag: \[0.40197912 0.03460513\] \[-0.01053336\]

nearing flag: \[0.43560933 0.03363021\] \[-0.05525178\]

nearing flag: \[0.46858119 0.03297186\] \[-0.00404176\]

reached flag on mountain! \[0.46858119 0.03297186\] \[-0.00404176\]

values at done: \[0.46858119 0.03297186\] \[-0.00404176\]

episode: 0 | episode reward: 85.0265 | time steps: 401

max\_pos: 0.46858118582758373 | max\_speed: 0.06316450856593404

----------------------------------------------------------------------

nearing flag: \[0.41649966 0.03903642\] \[-0.35404512\]

nearing flag: \[0.45439475 0.03789509\] \[-0.2345565\]

reached flag on mountain! \[0.45439475 0.03789509\] \[-0.2345565\]

values at done: \[0.45439475 0.03789509\] \[-0.2345565\]

episode: 1 | episode reward: 74.5208 | time steps: 760

max\_pos: 0.4543947454824115 | max\_speed: 0.06026767201058934

----------------------------------------------------------------------

nearing flag: \[0.40590966 0.03197592\] \[0.56079626\]

nearing flag: \[0.43793569 0.03202603\] \[0.6097038\]

nearing flag: \[0.46910794 0.03117225\] \[-0.14556652\]

reached flag on mountain! \[0.46910794 0.03117225\] \[-0.14556652\]

values at done: \[0.46910794 0.03117225\] \[-0.14556652\]

episode: 2 | episode reward: 72.0098 | time steps: 649

max\_pos: 0.46910794085174023 | max\_speed: 0.06396141357900643

----------------------------------------------------------------------
</code></pre></div></div>
:ET